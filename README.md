# Flash attention (v1) implementation


- Paper: https://arxiv.org/abs/2205.14135 


<br>


### Benchmark:
<h4> I compare the Flash Attention implementation with a pure pytorch implementation of the attention algorithm </h4>
- Pure torch attention CUDA time total: 64.838ms
<br>
- Flash Attention CUDA time total: 9.864ms